{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 表示文本\n",
        "\n",
        "如果我們想用神經網絡解決自然語言處理 (NLP) 任務，我們需要某種方式將文本表示為張量。計算機已經將文本字符表示為數字，使用 ASCII 或 UTF-8 等編碼映射到屏幕上的字體。\n",
        "\n",
        "![顯示將字符映射到 ASCII 和二進製表示的圖表的圖像](./img/ascii-character-map.png)\n",
        "\n",
        "我們了解每個字母 **代表** 的含義，以及所有字符如何組合在一起形成句子的單詞。但是，計算機本身並沒有這樣的理解，神經網絡必須在訓練時學習其含義。\n",
        "\n",
        "因此，我們可以在表示文本時使用不同的方法：\n",
        "* **字符級表示**，當我們通過將每個字符視為一個數字來表示文本時。鑑於我們的文本語料庫中有 $C$ 不同的字符，單詞 *Hello* 將由 $5\\times C$ 張量表示。每個字母將對應於 one-hot 編碼中的一個張量列。\n",
        "* **詞級表示**，其中我們創建了文本中所有單詞的**詞彙表**，然後使用 one-hot 編碼表示單詞。這種方法在某種程度上更好，因為每個字母本身並沒有多大意義，因此通過使用更高級別的語義概念——單詞——我們簡化了神經網絡的任務。然而，鑑於字典很大，我們需要處理高維稀疏張量。\n",
        "\n",
        "讓我們從安裝一些我們將在本模塊中使用的必需 Python 包開始。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gensim==3.8.3\n",
            "  Downloading gensim-3.8.3-cp38-cp38-macosx_10_9_x86_64.whl (24.2 MB)\n",
            "     |████████████████████████████████| 24.2 MB 4.1 MB/s            \n",
            "\u001b[?25hCollecting huggingface==0.0.1\n",
            "  Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/Caskroom/miniforge/base/envs/pydev/lib/python3.8/site-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (3.4.3)\n",
            "Collecting nltk==3.5\n",
            "  Downloading nltk-3.5.zip (1.4 MB)\n",
            "     |████████████████████████████████| 1.4 MB 6.0 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting numpy==1.18.5\n",
            "  Downloading numpy-1.18.5-cp38-cp38-macosx_10_9_x86_64.whl (15.1 MB)\n",
            "     |████████████████████████████████| 15.1 MB 6.0 MB/s            \n",
            "\u001b[?25hCollecting opencv-python==4.5.1.48\n",
            "  Downloading opencv_python-4.5.1.48-cp38-cp38-macosx_10_13_x86_64.whl (40.3 MB)\n",
            "     |████████████████████████████████| 40.3 MB 3.7 MB/s            \n",
            "\u001b[?25hCollecting Pillow==7.1.2\n",
            "  Downloading Pillow-7.1.2-cp38-cp38-macosx_10_10_x86_64.whl (2.2 MB)\n",
            "     |████████████████████████████████| 2.2 MB 3.4 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/Caskroom/miniforge/base/envs/pydev/lib/python3.8/site-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 8)) (1.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/Caskroom/miniforge/base/envs/pydev/lib/python3.8/site-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 9)) (1.7.1)\n",
            "Collecting torch==1.8.1\n",
            "  Downloading torch-1.8.1-cp38-none-macosx_10_9_x86_64.whl (119.6 MB)\n",
            "     |████████████████████████████████| 119.6 MB 4.3 MB/s            \n",
            "\u001b[?25hCollecting torchaudio==0.8.1\n",
            "  Downloading torchaudio-0.8.1-cp38-cp38-macosx_10_9_x86_64.whl (1.5 MB)\n",
            "     |████████████████████████████████| 1.5 MB 27.6 MB/s            \n",
            "\u001b[?25hCollecting torchinfo==0.0.8\n",
            "  Downloading torchinfo-0.0.8-py3-none-any.whl (16 kB)\n",
            "Collecting torchtext==0.9.1\n",
            "  Downloading torchtext-0.9.1-cp38-cp38-macosx_10_9_x86_64.whl (1.6 MB)\n",
            "     |████████████████████████████████| 1.6 MB 4.9 MB/s            \n",
            "\u001b[?25hCollecting torchvision==0.9.1\n",
            "  Downloading torchvision-0.9.1-cp38-cp38-macosx_10_9_x86_64.whl (13.2 MB)\n",
            "     |████████████████████████████████| 13.2 MB 5.2 MB/s            \n",
            "\u001b[?25hCollecting transformers==4.3.3\n",
            "  Downloading transformers-4.3.3-py3-none-any.whl (1.9 MB)\n",
            "     |████████████████████████████████| 1.9 MB 5.4 MB/s            \n",
            "\u001b[?25hCollecting smart-open>=1.8.1\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "     |████████████████████████████████| 58 kB 5.2 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /usr/local/Caskroom/miniforge/base/envs/pydev/lib/python3.8/site-packages (from gensim==3.8.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/Caskroom/miniforge/base/envs/pydev/lib/python3.8/site-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (8.0.3)\n",
            "Requirement already satisfied: joblib in /usr/local/Caskroom/miniforge/base/envs/pydev/lib/python3.8/site-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (1.1.0)\n",
            "Collecting regex\n",
            "  Downloading regex-2021.11.10-cp38-cp38-macosx_10_9_x86_64.whl (288 kB)\n",
            "     |████████████████████████████████| 288 kB 6.6 MB/s            \n",
            "\u001b[?25hCollecting tqdm\n",
            "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
            "     |████████████████████████████████| 76 kB 4.7 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/Caskroom/miniforge/base/envs/pydev/lib/python3.8/site-packages (from torch==1.8.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 10)) (3.7.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/Caskroom/miniforge/base/envs/pydev/lib/python3.8/site-packages (from torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (2.25.1)\n",
            "Requirement already satisfied: packaging in /usr/local/Caskroom/miniforge/base/envs/pydev/lib/python3.8/site-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (21.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "     |████████████████████████████████| 895 kB 5.4 MB/s            \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp38-cp38-macosx_10_11_x86_64.whl (2.2 MB)\n",
            "     |████████████████████████████████| 2.2 MB 6.5 MB/s            \n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/Caskroom/miniforge/base/envs/pydev/lib/python3.8/site-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/Caskroom/miniforge/base/envs/pydev/lib/python3.8/site-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/Caskroom/miniforge/base/envs/pydev/lib/python3.8/site-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/Caskroom/miniforge/base/envs/pydev/lib/python3.8/site-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (2.4.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/Caskroom/miniforge/base/envs/pydev/lib/python3.8/site-packages (from scikit-learn->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 8)) (3.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/Caskroom/miniforge/base/envs/pydev/lib/python3.8/site-packages (from requests->torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (2021.10.8)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/Caskroom/miniforge/base/envs/pydev/lib/python3.8/site-packages (from requests->torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/Caskroom/miniforge/base/envs/pydev/lib/python3.8/site-packages (from requests->torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (1.26.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/Caskroom/miniforge/base/envs/pydev/lib/python3.8/site-packages (from requests->torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (2.10)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434692 sha256=64a5839c8061049f7ce7351c5a600cd43b46b1c8346a6c24a303d66982067316\n",
            "  Stored in directory: /Users/lokinfey/Library/Caches/pip/wheels/ff/d5/7b/f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155\n",
            "Successfully built nltk\n",
            "Installing collected packages: tqdm, regex, numpy, torch, tokenizers, smart-open, sacremoses, Pillow, filelock, transformers, torchvision, torchtext, torchinfo, torchaudio, opencv-python, nltk, huggingface, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 8.3.2\n",
            "    Uninstalling Pillow-8.3.2:\n",
            "      Successfully uninstalled Pillow-8.3.2\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.5.0\n",
            "    Uninstalling torchvision-0.5.0:\n",
            "      Successfully uninstalled torchvision-0.5.0\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.4.0a0+ba6d67b\n",
            "    Uninstalling torchaudio-0.4.0a0+ba6d67b:\n",
            "      Successfully uninstalled torchaudio-0.4.0a0+ba6d67b\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-macos 2.6.0 requires numpy~=1.19.2, but you have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "Successfully installed Pillow-7.1.2 filelock-3.4.0 gensim-3.8.3 huggingface-0.0.1 nltk-3.5 numpy-1.18.5 opencv-python-4.5.1.48 regex-2021.11.10 sacremoses-0.0.46 smart-open-5.2.1 tokenizers-0.10.3 torch-1.8.1 torchaudio-0.8.1 torchinfo-0.0.8 torchtext-0.9.1 torchvision-0.9.1 tqdm-4.62.3 transformers-4.3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 文本分類任務\n",
        "\n",
        "在本模塊中，我們將從基於 **AG_NEWS** 數據集的簡單文本分類任務開始，該任務將新聞標題分類為 4 個類別之一：World、Sports、Business 和 Sci/Tech。 該數據集內置於 [`torchtext`](https://github.com/pytorch/text) 模塊中，因此我們可以輕鬆訪問它。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train.csv: 29.5MB [00:01, 15.0MB/s]\n",
            "test.csv: 1.86MB [00:00, 4.46MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import os\n",
        "import collections\n",
        "os.makedirs('./data',exist_ok=True)\n",
        "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
        "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "這裡，`train_dataset` 和 `test_dataset` 包含迭代器，它們分別返回標籤對（類別數）和文本，例如："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3,\n",
              " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "因此，讓我們打印出數據集中的前 10 個新標題："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Sci/Tech** -> Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
            "**Sci/Tech** -> Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
            "**Sci/Tech** -> Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
            "**Sci/Tech** -> Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n",
            "**Sci/Tech** -> Stocks End Up, But Near Year Lows (Reuters) Reuters - Stocks ended slightly higher on Friday\\but stayed near lows for the year as oil prices surged past  #36;46\\a barrel, offsetting a positive outlook from computer maker\\Dell Inc. (DELL.O)\n"
          ]
        }
      ],
      "source": [
        "for i,x in zip(range(5),train_dataset):\n",
        "    print(f\"**{classes[x[0]]}** -> {x[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because datasets are iterators, if we want to use the data multiple times we need to convert it to list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
        "train_dataset = list(train_dataset)\n",
        "test_dataset = list(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "現在我們需要將文本轉換為可以表示為張量的**數字**。 如果我們想要詞級表示，我們需要做兩件事：\n",
        "* 使用 **tokenizer** 將文本拆分為 **tokens**\n",
        "* 構建這些標記的 **詞彙**。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['he', 'said', 'hello']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
        "tokenizer('He said: hello')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "counter = collections.Counter()\n",
        "for (label, line) in train_dataset:\n",
        "    counter.update(tokenizer(line))\n",
        "vocab = torchtext.vocab.Vocab(counter, min_freq=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "使用詞彙表，我們可以輕鬆地將標記化的字符串編碼為一組數字："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size if 95812\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[283, 2321, 5, 337, 19, 1301, 2357]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size = len(vocab)\n",
        "print(f\"Vocab size if {vocab_size}\")\n",
        "\n",
        "def encode(x):\n",
        "    return [vocab.stoi[s] for s in tokenizer(x)]\n",
        "\n",
        "encode('I love to play with my words')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 詞袋文本表示\n",
        "\n",
        "因為單詞代表意義，有時我們可以通過查看單個單詞來理解文本的含義，而不管它們在句子中的順序如何。 例如，在對新聞進行分類時，像*weather*、*snow* 這樣的詞可能表示*weather forecast*，而像*stocks*、*dollar* 這樣的詞將計入*financial news*。\n",
        "\n",
        "**詞袋**（BoW）向量表示是最常用的傳統向量表示。 每個詞都鏈接到一個向量索引，向量元素包含一個詞在給定文檔中出現的次數。\n",
        "\n",
        "![圖像顯示了詞袋向量表示如何在內存中表示。](./img/bag-of-words-example1.png)\n",
        "\n",
        "> **注意**：您還可以將 BoW 視為文本中單個單詞的所有單熱編碼向量的總和。\n",
        "\n",
        "下面是如何使用 Scikit Learn python 庫生成詞袋表示的示例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 2, 0, 0, 0, 0, 0]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "corpus = [\n",
        "        'I like hot dogs.',\n",
        "        'The dog ran fast.',\n",
        "        'Its hot outside.',\n",
        "    ]\n",
        "vectorizer.fit_transform(corpus)\n",
        "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "要從 AG_NEWS 數據集的向量表示中計算詞袋向量，我們可以使用以下函數："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0., 0., 2.,  ..., 0., 0., 0.])\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "def to_bow(text,bow_vocab_size=vocab_size):\n",
        "    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n",
        "    for i in encode(text):\n",
        "        if i<bow_vocab_size:\n",
        "            res[i] += 1\n",
        "    return res\n",
        "\n",
        "print(to_bow(train_dataset[0][1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **注意：** 這裡我們使用全局`vocab_size` 變量來指定詞彙表的默認大小。 由於通常詞彙量非常大，我們可以將詞彙量限制為最常用的詞。 嘗試降低 `vocab_size` 值並運行下面的代碼，看看它如何影響準確性。 您應該期待一些準確度下降，但不會顯著下降，以代替更高的性能。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 訓練 BoW 分類器\n",
        "\n",
        "現在我們已經學會瞭如何構建文本的詞袋表示，讓我們在它之上訓練一個分類器。 首先，我們需要以這種方式轉換我們的訓練數據集，將所有位置向量表示轉換為詞袋表示。 這可以通過將 `bowify` 函數作為 `collate_fn` 參數傳遞給標準 Torch `DataLoader` 來實現："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import numpy as np \n",
        "\n",
        "# this collate function gets list of batch_size tuples, and needs to \n",
        "# return a pair of label-feature tensors for the whole minibatch\n",
        "def bowify(b):\n",
        "    return (\n",
        "            torch.LongTensor([t[0]-1 for t in b]),\n",
        "            torch.stack([to_bow(t[1]) for t in b])\n",
        "    )\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=bowify, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=bowify, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "現在讓我們定義一個包含一個線性層的簡單分類器神經網絡。 輸入向量的大小等於`vocab_size`，輸出大小對應於類的數量（4）。 因為我們是在解決分類任務，所以最終的激活函數是`LogSoftmax()`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "net = torch.nn.Sequential(torch.nn.Linear(vocab_size,4),torch.nn.LogSoftmax(dim=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "現在我們將定義標準的 PyTorch 訓練循環。 因為我們的數據集非常大，出於教學目的，我們只會訓練一個 epoch，有時甚至少於一個 epoch（指定 epoch_size 參數允許我們限制訓練）。 我們還會在訓練過程中報告累積的訓練準確率； 報告頻率是使用`report_freq` 參數指定的。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.NLLLoss(),epoch_size=None, report_freq=200):\n",
        "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
        "    net.train()\n",
        "    total_loss,acc,count,i = 0,0,0,0\n",
        "    for labels,features in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        out = net(features)\n",
        "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss+=loss\n",
        "        _,predicted = torch.max(out,1)\n",
        "        acc+=(predicted==labels).sum()\n",
        "        count+=len(labels)\n",
        "        i+=1\n",
        "        if i%report_freq==0:\n",
        "            print(f\"{count}: acc={acc.item()/count}\")\n",
        "        if epoch_size and count>epoch_size:\n",
        "            break\n",
        "    return total_loss.item()/count, acc.item()/count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3200: acc=0.8084375\n",
            "6400: acc=0.84328125\n",
            "9600: acc=0.8559375\n",
            "12800: acc=0.863046875\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.026000787454373293, 0.8663379530916845)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_epoch(net,train_loader,epoch_size=15000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BiGrams、TriGrams 和 N-Grams\n",
        "\n",
        "詞袋方法的一個限制是某些詞是多詞表達的一部分，例如，“熱狗”一詞與其他上下文中的詞“熱”和“狗”具有完全不同的含義。 如果我們總是用相同的向量表示單詞“hot”和“dog”，它會混淆我們的模型。\n",
        "\n",
        "為了解決這個問題，**N-gram 表示**經常用於文檔分類方法，其中每個詞、雙詞或三詞的頻率是訓練分類器的有用特徵。 例如，在雙元組表示中，除了原始單詞之外，我們還會將所有單詞對添加到詞彙表中。\n",
        "\n",
        "下面是一個如何使用 Scikit Learn 生成詞表示的二元詞袋的示例：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary:\n",
            " {'i': 7, 'like': 11, 'hot': 4, 'dogs': 2, 'i like': 8, 'like hot': 12, 'hot dogs': 5, 'the': 16, 'dog': 0, 'ran': 14, 'fast': 3, 'the dog': 17, 'dog ran': 1, 'ran fast': 15, 'its': 9, 'outside': 13, 'its hot': 10, 'hot outside': 6}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
        "corpus = [\n",
        "        'I like hot dogs.',\n",
        "        'The dog ran fast.',\n",
        "        'Its hot outside.',\n",
        "    ]\n",
        "bigram_vectorizer.fit_transform(corpus)\n",
        "print(\"Vocabulary:\\n\",bigram_vectorizer.vocabulary_)\n",
        "bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "N-gram 方法的主要缺點是詞彙量開始增長得非常快。 在實踐中，我們需要將 N-gram 表示與一些降維技術相結合，例如 *embeddings*，我們將在下一個單元中討論。\n",
        "\n",
        "要在我們的 **AG News** 數據集中使用 N-gram 表示，我們需要構建特殊的 ngram 詞彙表："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bigram vocabulary length =  1308844\n"
          ]
        }
      ],
      "source": [
        "counter = collections.Counter()\n",
        "for (label, line) in train_dataset:\n",
        "    l = tokenizer(line)\n",
        "    counter.update(torchtext.data.utils.ngrams_iterator(l,ngrams=2))\n",
        "    \n",
        "bi_vocab = torchtext.vocab.Vocab(counter, min_freq=1)\n",
        "\n",
        "print(\"Bigram vocabulary length = \",len(bi_vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "然後我們可以使用與上面相同的代碼來訓練分類器，但是，它的內存效率非常低。 在下一個單元中，我們將使用嵌入訓練二元分類器。\n",
        "\n",
        "> **注意：** 您只能保留在文本中出現次數超過指定次數的那些 ngram。 這將確保忽略不常見的二元組，並顯著降低維度。 為此，將 `min_freq` 參數設置為更高的值，並觀察詞彙變化的長度。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 詞頻逆文檔頻率TF-IDF\n",
        "\n",
        "在 BoW 表示中，無論單詞本身如何，單詞出現的權重都是均勻的。但是，很明顯，與專業術語相比，*a*、*in* 等頻繁出現的詞對分類的重要性要小得多。事實上，在大多數 NLP 任務中，有些詞比其他詞更相關。\n",
        "\n",
        "**TF-IDF** 代表**詞頻-逆文檔頻率**。它是詞袋的一種變體，其中使用浮點值而不是指示單詞在文檔中出現的二進制 0/1 值，該值與語料庫中單詞出現的頻率有關。\n",
        "\n",
        "更正式地，文檔$j$中單詞$i$的權重$w_{ij}$定義為：\n",
        "$$\n",
        "w_{ij} = tf_{ij}\\times\\log({N\\over df_i})\n",
        "$$\n",
        "在哪裡\n",
        "* $tf_{ij}$是$j$中$i$出現的次數，即我們之前看到的BoW值\n",
        "* $N$ 是集合中的文檔數\n",
        "* $df_i$ 是整個集合中包含單詞 $i$ 的文檔數\n",
        "\n",
        "TF-IDF 值 $w_{ij}$ 與單詞在文檔中出現的次數成正比增加，並被包含該單詞的語料庫中的文檔數所抵消，這有助於調整某些單詞出現的事實比其他人更頻繁。例如，如果該詞出現在集合中的*每個* 文檔中，$df_i=N$ 和 $w_{ij}=0$，這些詞將被完全忽略。\n",
        "\n",
        "您可以使用 Scikit Learn 輕鬆創建文本的 TF-IDF 矢量化："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.43381609, 0.        , 0.43381609, 0.        , 0.65985664,\n",
              "        0.43381609, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
        "vectorizer.fit_transform(corpus)\n",
        "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "然而，即使 TF-IDF 表示為不同的單詞提供頻率權重，它們也無法表示含義或順序。 正如著名語言學家 J. R. Firth 在 1935 年所說的那樣，“一個詞的完整意義總是與語境相關的，沒有語境就不能認真研究意義。”。 我們將在後面的單元中學習如何使用語言建模從文本中捕獲上下文信息。\n"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "eec8323f1fd0687cc16c170f83e009bfe267a929289f6367d1c75a585500211d"
    },
    "kernelspec": {
      "display_name": "py37_pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
